\section{Planteamiento y algunas observaciones}

Como ya hemos explicado anteriormente, el problema de \textit{string matching} se trata de buscar
todas las ocurrencias de una cadena no vacía, que se le denominará \textbf{patrón} en una cadena
llamada \textbf{texto}. 

La terminología usada en todo este capítulo será:
\begin{itemize}
    \item \texttt{ws} que denotará al patrón y
    \item \texttt{xs} el texto.
\end{itemize}

Como se mencionó en la motivación de este trabajo, lo que se hará es dar una especificación en
Haskell para después irlo ``mejorando''. Empecemos con esta especificación,

\begin{minted}{haskell}
matches :: Eq a => [a] -> [a] -> [Int]
matches ws = map length . filter (endswith ws) . inits  -- (ec.91)
\end{minted}

% TODO, explicar filter, length y map. Al final la composición de todo

La función \hsCode{inits} regresa una lista con los prefijos de una lista en orden creciente como se 
puede ver a continuación,

\begin{minted}{haskell}
>>> inits [2,3,5]
    [[],[2],[2,3],[2,3,5]]
\end{minted}

Donde \hsCode{inits} está definida como:

\inputminted{haskell}{definiciones/inits.hs}

La función \hsCode{endswith ws xs} verifica si el patón \texttt{ws} es sufijo de \texttt{xs}. Más adelante esta función será definida formalmente.

El valor de \hsCode{matches ws xs} es una lista de enteros $p$ tal que \texttt{ws} es un sufijo de \hsCode{take p xs}. Por ejemplo

\begin{minted}{haskell}
>>> matches "abcab" "ababcabcab" 
    [7, 10]
\end{minted}

En otras palabra, \hsCode{matches ws xs} regresa una lista de enteros donde cada entero $p$ indica que
\texttt{ws} aparece en \texttt{xs} terminando en la posición $p$ (siendo 1-indexado).

La función \hsCode{matches} es polimórfica, así que cualquier algoritmo del problema tiene que
depender de la función de comparación \hsCode{(==) :: a -> a -> Bool} sobre elementos de dos listas.
Asumiendo que toma tiempo constante hacer la función de comparación, la complejidad en tiempo de
\hsCode{matches ws xs} es de $\Theta(mn)$ pasos en el peor de los casos, donde $m$ el la longitud de \texttt{ws} y $n$ la
de \texttt{xs}.

Nuestra meta aquí es deerviar el algoritmo KMP para emparejamiento de cadenas, el cual reduce el tiempo a $\Theta(m+n)$ pasos.
Y aquí el ``truco'' a utilizar es aplicar estrategias como leyes de fusión, tuple %% poner todas las estrategias que usaré
para poder manipular la expresión %% TODO: aquí es donde tengo que poner las venttajas de la programación funciional y su manipulación algebraica.

Así un emparejamiento (\textit{match}) ocurre en la posición $p$ si el patrón \texttt{ws} aparece en el texto \texttt{xs} terminando en $p$.


Siempre hay que tener en cuenta que cualquier problema que involucre la función \hsCode{inits}, la ley más importante es conocida como \textbf{Scan Lemma}: %% TODO: referirme al capittulo donde demuestre esto
\begin{minted}{haskell}
map (foldl op e) . inits = scanl op e  -- (ec.92)
\end{minted}

¿Por qué usar \textbf{Scan Lemma}? En pocas pabras para mejorar la complejidad en tiempo evaluando menos expresiones.
Porque en (ec.92), en la expresión LHS es evaluado en una lista de longitud $n$ con $\Theta(n^2)$ evaluaciones de \hsCode{op}, mientras que
en RHS en la expresión equivalente en la función estándar de Haskell \hsCode{scanl} solo requiere $\Theta(n)$ evaluaciones.

Aunque hay un \hsCode{map} en la definición de \hsCode{matches}, también hay un \hsCode{filter}, así que el primer paso en tranformar \hsCode{matches} is reescribir la especificación usando otra ley: %% TODO: hacer una referencia a esta ley y demostrarla

\begin{minted}{haskell}
map f . filter p = map fst . filter snd . map (fork (f,p))  -- (ec.93)
\end{minted}

donde \hsCode{fork (f, p) x = (f x, p x)}. La ley usada simplemente es para poner a \hsCode{map}
después de \hsCode{inits} para prepararlo para aplicar el \textit{scan lemma}. Usando (ec.93) nos lleva a

\begin{minted}{haskell}
matches ws
  = map length . filter (endswith ws)  -- (ec.91)
  = map fst . filter snd . map (fork (length , endswith ws)) . inits  -- Por (e.93)
\end{minted}

La siguiente pregunta a responder es: ¿Puede \hsCode{fork (length, endswith ws)} puede ser convertido
como una instancia de \hsCode{fold}? Ciértamente \hsCode{length = foldl count 0} donde
\hsCode{count n x = n + 1} % TODO: Detallar aquí porqué, caso base e inducción 
Ahora veamos como podemos escribir a \hsCode{endswith ws} como una instancia de \hsCode{foldl}.
Supongamos que se pueden encontrar \hsCode{e} y \hsCode{op}, los cuales dependen de \texttt{ws} tal que,

\begin{minted}{haskell}
endswith ws = foldl op e  -- (ec.94)
\end{minted}

Entonces estamos en posición de aplicar otra ley: la \textit{tupling law} para \hsCode{foldl}. La ley establece que,

\begin{minted}{haskell}
fork (foldl op1 e1, foldl op2 e2) = foldl op (e1, e2)  -- (ec.95)
\end{minted}

donde \hsCode{op (a, b) x = (op1 a x, op2 b x) -- (op.96)}. Usando la \textit{tupling law} resulta en

\begin{minted}{haskell}
fork (foldl op1 e1, foldl op2 e2) = foldl op (e1, e2)  -- Usando (ec.95)
fork (foldl count 0, foldl op e) = foldl step (0, e)  -- Por (ec.100) y (ec.94)
fork (length, endswith ws) = foldl step (0, e) -- (ec.100)
step (n, x) y = (n+1, op x y)  -- Por -- (ec.96)
\end{minted}
%% TODO: explicar  el dominio y contradominio de step y op

Finalnente se puede aplicar \textit{scan lemma} para llegar a,
\begin{minted}{haskell}
matches ws =
  map fst . filter snd . (map (fork (length , endswith ws)) . inits)
  -- { De la (ec.100) se sigue 
  --   fork (length , endswith ws) = foldl step (0, e) }
  map fst . filter snd . scanl step (0, e) -- Sutituyendo -- (ec.103)
\end{minted}

% Esto lo puse yo
Porque de (ec.40) lo mezclo con (ec.42)
\begin{minted}{haskell}
matches ws = (map length . filter (endswith ws)) . inits
matches ws = (map fst . filter snd . map (fork (f,p))) . inits
\end{minted}

Si \hsCode{op} toma tiempo constante o al menos amortizada, entonces también \hsCode{step}, y el resultado es tiempo lineal.

El problema ahora es que no hay \hsCode{e} y \hsCode{op} que satisfagan (ec.94). La función de \hsCode{endswith ws}
regresa un valor \hsCode{Bool} y eso es información insuficiente para expresarlo como una instancia de
\hsCode{foldl}.
La sieguiente mejor cosa a hacer es expresar a \hsCode{endswith ws} como una composición.

\begin{minted}{haskell}
endswith ws = p . foldl op e  -- (ec.101)
\end{minted}

La forma de (ec.101) está puesta de esta manera solamente para aplicar el \textit{scan lemma}. 
En ves de (ec.93) podemos decudir una ligera generalización:
\begin{minted}{haskell}
map f . filter (p . g) = map fst . filter (p. snd) . map (fork (f,g))  -- (ec.102)
\end{minted}
%% TODO: demostrar

Entonces se obtiene
\begin{minted}{haskell}
matches ws = map fst . filter (p . snd) . scanl step (0,e) -- (ec.103)
\end{minted} 
%% TODO: ver si puedo poner pasos iintermedios

Dadas \hsCode{p} y \hsCode{op} que tomen tiempo constante amortizado, \hsCode{matches} seguirá tomando tiempo lineal.

Lo que queda por encontrar es \hsCode{p}, \hsCode{p} y \hsCode{e} que satisfagan (ec.101).

Pero antes de que sigamos avanzando, definamos a \hsCode{endswith} formalmente como
\begin{minted}{haskell}
endswith ws xs = ws ∈ tails xs
\end{minted}

% FIXME: Hacer el análisis formal de la complejidad.
%% Explicar como funciona tails, tomar la definición de la documentación
%Donde \hsCode{tails} está definida como, %% TODO acomodar esto
%\inputminted{haskell}{definiciones/tails.hs}

%\begin{minted}{haskell}
%>>> tails [2,3,5,7]
%    [[2,3,5,7],[3,5,7],[5,7],[7],[]]
%\end{minted}

\section{Introducción}

Por ahora se debe de tener en mente que se deben buscar funciones \hsCode{p} y \hsCode{op} y un
valor \hsCode{e}, tal que \hsCode{endswith ws = p . foldl op e  -- (ec.101)}.

Recordando la ecuación (ec.103) %% TODO: poner más bonito la definición y poner step y la firma

Donde \hsCode{matches ws xs} es una lista de enteros, en la cual cada entero $n$ dice si el patrón
\texttt{ws} aparece en el texto \texttt{xs} acabando en la posición $n$.

%% TODO: ver como lo quito porque está muy repetitvo con lo de arriba
Dadas \hsCode{p} y \hsCode{op} que tomen tiempo constante, o al menos tiempo constante amortizado,
\hsCode{matches} tomará $\Theta(n + m)$ pasos calcular las apariciones del patrón en el texto, donde
el patrón tiene una longitud de $m$ y el texto una longitud de $n$.

%% TODO: incluso ver si quitto el capítulo de introducción para juntarlo con lo primero

\section{Primer acercamiento}

Se puede escribir a \hsCode{endswith ws} como una composición funciones,
%% TODO: explicar porque lo escribí así
\begin{minted}{haskell}
endswith ws = not . null . filter (= ws) . tails
\end{minted}

Pero no es una buena idea definir a \hsCode{filter (= ws) . tails} con una función de plegado
\hsCode{foldl} porque regresaria o bien una lista vacia o \hsCode{[ws]}, lo cual sería información
insuficiente para definir ésa función inductivamente.

Para ejemplificar esto, hagamos esa función,
\begin{minted}{haskell}
f ws = foldl (\ys xs -> if xs == ws then xs:ys else ys) [] . tails
\end{minted}

\begin{minted}{haskell}
>>> f "nea" "Atenea"
    ["nea"]

>>> f "ene" "Atenea"
    []
\end{minted}

Consideremos la función prefijo $(\sqsubseteq)$ definida como:

\inputminted{haskell}{definiciones/prefix.hs}

Teniendo la función prefijo $(\sqsubseteq)$, se ve más prometedora la función
\hsCode{filter (⊑ ws) . tails} porque se podría obtener más ``información'' inductivamente.

Aplicando \texttt{xs} a la función \hsCode{filter (⊑ ws) . tails}, regresará en orden decreciente
sobre la longitud de todas la colas de \texttt{xs}, todas las que sean prefijos de \texttt{ws}.

% TODO: Poner ejemplo

Entonces, el primer elemento de la lista es \texttt{ws} si y solo si \hsCode{endswith ws xs} es
verdadero, porque como \hsCode{tails} regresa las colas en orden decreciente y si \texttt{ws} no 
fuera el primer elemento entonces había otra \texttt{ws'} con una longitud menor a \texttt{ws}, lo
cual no podría ser.

De ahí que se puede definir a \hsCode{endswith} como una composición de la composición de funciones
\hsCode{filter (⊑ ws) . tails} propuesta anteriormente como,

\begin{minted}{haskell}
endswith ws = (= ws) . head . filter (⊑ ws) . tails
\end{minted}

La primera función \hsCode{(= ws)} no es función que tome tiempo constante (por obvias razones). 
Ese problema es resulto generalizando la función \hsCode{filter (⊑ ws) . tails} a una función
\hsCode{split} definida como,

\begin{minted}{haskell}
split ws xs = head [(us, ws ↓ us) | us <- tails xs, us ⊑ ws]
\end{minted}

La función \hsCode{(↓)} de forma abstracta está definida como \hsCode{(us ++ vs) ↓ us = vs} y
concrétamente como,

\inputminted{haskell}{definiciones/kmp/1-down-arrow.hs}

De ahí que \hsCode{split ws xs} separa \texttt{ws} en dos listas \texttt{us} y \texttt{vs} tal que
\texttt{us ++ vs = ws}.

\begin{center}
\textbf{El valor de \texttt{us} es el sufijo más largo de \texttt{xs} que es prefijo de \texttt{ws}.}
\end{center}

Donde \texttt{ws = "{}endnote"} y \texttt{xs = "{}append"}, y en \texttt{ws} el pefijo más largo
es \texttt{"\color{purple}{}end\color{black}note"} y en \texttt{xs} el sufijo es
\texttt{"{}app\color{purple}end\color{black}"}.

Por ejemplo,
\begin{minted}{haskell}
>>> split "endnote" "append"
    head [(us, ws ↓ us) | us <- tails "append", us ⊑ "endnote"]
    head [(us, ws ↓ us) | us <- ["append","ppend","pend","end",
                                 "nd","d",""],
                          us ⊑ "endnote"]
    head [("end","note"), ("","endnote")]
    ("end","note")
\end{minted}

Recordando que la función \hsCode{null . snd} regresa \hsCode{Bool} dependiendo si el segundo valor
de una tupla es vacío o no. Se utilizará compuesta con \hsCode{split ws} para indicar que si se
puede separar \texttt{ws} como se enseñó anteriormente, y así indicar si \texttt{ws} es un sufijo
de \texttt{xs}. Teniendo así que \hsCode{endswith ws = null . snd . split ws}.

Queda a encontrar \hsCode{op} y \hsCode{e} tal que \hsCode{split ws = foldl op e}. De forma
equivalente, se quiere encontrar \hsCode{e} y \hsCode{op} tal que satisfagan, 

%% TODO: como estte es un patrón comun de foldl, explicarlo
\inputminted{haskell}{definiciones/kmp/1-split-eq.hs}

Se tiene que \hsCode{split ws [] = ([], ws)} porque, 
\begin{minted}{haskell}
split ws []
= head [(us, ws ↓ us) | us <- tails [], us ⊑ ws]
= head [(us, ws ↓ us) | us <- [[]], us ⊑ ws]
= head [([], ws ↓ []) | [] ⊑ ws]
= ([], ws)
\end{minted}

teniendo así que \hsCode{e = ([], ws)}.

Faltando solo descubrir \hsCode{op}. La siguiente observación es crucial,

\hsCode{split ws xs = (us,vs)} $\quad\Longrightarrow\quad$ 
\hsCode{split ws (xs ++ [x]) = split ws (us ++ [x])  -- (ec.3)}

Que se interpreta como, el sufijo más largo de \hsCode{xs ++ [x]} es un prefijo de \hsCode{ws} es un
sufijo de \hsCode{us ++ [x]}.

Para descubir \hsCode{op}, primero se debe expresar a \hsCode{split} recursivamente como,
\begin{minted}{haskell}
split ws xs = if xs ⊑ ws then (xs, ws ↓ xs) else split ws (tail xs)  -- (ec.4)
\end{minted}

Como \hsCode{split ws xs = (us, vs)} donde\hsCode{ ws = us ++ vs} por definición de \hsCode{split},
se hace el razonamiento:

\inputminted{haskell}{definiciones/kmp/1-op-raz-eq.hs}

Y todo ese cálculo nos da la definición de \hsCode{op :: ([a], [a]) -> a -> ([a], [a])} como:

\inputminted{haskell}{definiciones/kmp/1-op.hs}

% TODO: Explicar porque
% split ws (tail us ++ [x]) = op (split ws (tail us)) x
Quedando así (la primera versión) hasta ahora como:

\inputminted{haskell}{definiciones/kmp/1-final.hs}

% TODO poner la explicación que falta justo antes de Data refinement

% Poner el ejmeplo y su ejecución

Juntando todo lo anterior, el código del primer acercamiento quedaría como:

\inputminted{haskell}{codigo/haskell/1-first-steps.hs}

% FIXME: Explicar el funcionamiento, está justo arriba de first steps

\section{Refinamiento de los datos}

Una manera de mejorar la eficiencia, es buscar un cambio en la representación del primer argumento 
de \hsCode{op}, es decir la separación \texttt{(us, vs)} actual sobre el patrón \texttt{ws}.

Supongamos funciones \hsCode{abs} y \hsCode{rep} con tipos,

\begin{minted}{haskell}
abs :: Rep ([a], [a]) -> ([a], [a])
rep :: ([a], [a]) -> Rep ([a], [a])
\end{minted}

para algún tipo de dato \hsCode{Rep}.

La función \hsCode{rep} es la función de representación, es decir, ``encapsula'' \texttt{([a],[a])},
mientras que \hsCode{abs} es la función de abstracción. % TODO: Del último paper que encontré en twitter argumentar esto.

Se quiere también que \texttt{abs . rep = id}, es decir \hsCode{abs} es la inversa derecha de
\hsCode{rep}. Esta condición establece que el valor abstracto puede ser recuperado de cualquier
representación de esta.

Si se encontrarán las siguientes funciones que asegurarán que,
\begin{minted}{haskell}
foldl op ([], ws) = abs . foldl op' (rep ([], ws))  -- (ec.5)
\end{minted}

Y también si aseguramos que \hsCode{abs} y \hsCode{op'} toman tiempo constante, entonces se puede
redefinir \hsCode{matches} como,

\begin{minted}{haskell}
matches ws = map fst . filter (null . snd . abs . snd) .
             scanl step (0, rep ([], ws))
step (n, r) x = (n + 1, op' r x)    
\end{minted}

% TODO: Ver si se traduce así fusion law, hacer como una liga a cada una de las tres condiciones necesarias
Para encontrar \hsCode{abs}, \hsCode{op'} y \hsCode{rep} que satisfagan (ec.5) recurriremos a la ley de fusión de \hsCode{fold}.
% TODO: Poner cada una de las leyes de fusión del libro y completarlo con info extra.
Para la primera condición no se necesita si queremos afirmar que la ley de fusión se mantiene solo para todas las listas finitas.
La cosa interesante aquí es que que queremos aplicar la ley de fusión para ``separar'', dividiendo el \hsCode{fold} en dos partes.

La segunda condición es inmediata porque,
\begin{minted}{haskell}
abs (rep ([],ws)) = (abs . rep) ([],ws) = id ([],ws) = ([],ws)
\end{minted}

Se dará una definición de \hsCode{op'} que satisface la tercera condición de fusión,
específicamente donde \texttt{r} es de tipo \hsCode{Rep},
\begin{minted}{haskell}
op' r = rep . op (abs r)  -- (ec.6)
\end{minted}

Entonces se tiene que,
\begin{minted}{haskell}
abs (op' r x) 
  = abs (rep (op (abs r) x))    -- Sustitución usando (ec.6)
  = (abs . rep) (op (abs r) x)  -- Composición
  = op (abs r) x                -- (ec.7)
\end{minted}

Ajustando la definición de \hsCode{op} en (ec.6) se obtiene,
\begin{minted}{haskell}
op' r x                                           -- (ec.8)
  | [x] ⊑ vs = rep (us ++ [x], tail vs)
  | null us = rep ([],ws)
  | otherwise = op' (rep (split ws (tail us))) x
    where (us, vs) = abs r  
\end{minted}

Con \hsCode{op' :: Rep ([a], [a]) -> a -> Rep ([a], [a])}. Teniendo esas representaciones
abtractas, aún queda escoger \hsCode{Rep} y las dos funciones \hsCode{abs} y \hsCode{rep}.

\section{Árboles}
% TODO: Poner como referencia lo de los functional arrays Okasaki
En la programación funcional, muchas representaciones eficiente involucran a un ábrol de cierto
tipo, y en este caso no es diferente. Se define,
\begin{minted}{haskell}
data Rep a = Null | Node a (Rep a) (Rep a)
\end{minted}

Teniendo así que \hsCode{Rep} es un árbol binario.

La función \hsCode{abs} está definida por,
\begin{minted}{haskell}
abs (Node (us, vs) lr) = (us, vs)  -- (ec.9)
\end{minted}

Y claramente toma tiempo constante. La función \hsCode{rep} está definida por,

\begin{minted}{haskell}
rep (us, vs) = Node (us, vs) (left us vs) (right us vs)  -- (ec.10)
\end{minted}

donde,

\begin{minted}{haskell}
left [] vs      = Null                 -- (ec.11.1)
left (u:us) vs  = rep (split ws us)    -- (ec.11.2)
right us []     = Null                 -- (ec.12.1)
right us (v:vs) = rep (us ++ [v], vs)  -- (ec.12.2)
\end{minted}

La razón de elegir \hsCode{rep} en la (ec.10) es que \hsCode{op'} puede tomar una forma más simple.
Por análisis de casos en cada caso de \hsCode{op'} de (ec.8):

\begin{itemize}
\item Caso \hsCode{[x] ⊑ vs}
\begin{minted}{haskell}
op' (Node (us, vs) l r) x
  =  -- {Primer caso de `op'` (ec.8)}
rep (us ++ [x], tail vs)
  =  -- {Tomando el (RHS) de la definición de `right` en (ec.12.2) 
     --  y como vs = (v':vs'), `tail vs = vs'`}
right us vs
  =  -- {Por definición de `rep` (ec.10)}
r
\end{minted}

\item Caso \hsCode{null us}
\begin{minted}{haskell}
op' (Node (us, vs) l r) x
  =  -- {Segundo caso de `op'` (ec.8)}
rep ([], ws)
  = -- {Renombrando por simplicidad}
root
\end{minted}
Se debe tener en cuenta que ahora \hsCode{rep ([], ws) = root  -- (ec.13)}.

\item Caso \hsCode{otherwise}
\begin{minted}{haskell}
op' (Node (us, vs) l r) x
  =  -- {Tercer caso de `op'` (ec.8)}
op' (rep (split ws (tail us))) x
  =  -- {Tomando el (RHS) de la definición de `left` en (ec.11) y 
     --  como vs = (v':vs'), `tail vs = vs'`}
op' (left us vs) x
  =  -- {Por definición de `rep` (ec.10)}
op' l x
\end{minted}
\end{itemize}

Quedando así después de analizar los casos,
\begin{minted}{haskell}
op' (Node (us, vs) l r) x
  | [x] ⊑ vs  = r
  | null us   = root
  | otherwise = op' l x
\end{minted}

Si ponemos \hsCode{op' Null x = root} (más adelante se explicará el porqué) % TODO: explicar al final cuando se separa de (us,vs) => vs
, entonces \hsCode{op'} queda de una forma aún más sencilla:
\begin{minted}{haskell}
op' Null x                 = root
op' (Node (us, vs) l r) x  | [x] ⊑ vs  = r
                           | otherwise = op' l x
\end{minted}

Aunque \hsCode{op'} no toma tiempo constante, toma tiempo constante amortizado. % TODO: explicar por que?!
El árbol \hsCode{root} tiene altura $m$, la longitud del patrón; tomando la rama derecha decrementa la altura del árbol actual exactamente por uno, % TODO: explicar por que?!
mientras que tomando la rama izquiera incrementa la altura, posiblemente más de uno. % TODO: igual explicar
Un argumento estándar de amortízación ahora muestra que evaluando \hsCode{foldl op' root} en una lista de longitud $n$ involucra a lo más $2m + n$ llamadas de \hsCode{op'}. % TODO: explicar

Lo que queda descubir es como cálcular eficientemente \hsCode{rep}.
Es aquí cuando una técnica de transformación del programa entra en juego: % TODO: de los artículos que tengo argumentar esto mejor.
hacer uso de un parámetro de acumulación. %TODO: igual aquí explicar

La idea es dar una versión generalizanda de \hsCode{rep}, digamos \hsCode{grep}.

\begin{minted}{haskell}
rep (us, vs) = grep (left us vs) (us, vs)
\end{minted}

Y luego obtener una definición de \hsCode{grep}. De (ec.10) se tiene que

\begin{minted}{haskell}
grep l (us, vs) = Node (us, vs) l (right us vs)  -- (ec.14)
\end{minted}

\textbf{Análisis en \texttt{vs}:}
\begin{itemize}
\item Caso \texttt{vs = []}.
\begin{minted}{haskell}
right us []
  =  -- {Por definición de `right` (ec.12.1)}
Null
\end{minted}

\item Caso inductivo \texttt{(v:vs)}

\textbf{Análisis en \texttt{us}:}
\begin{minted}{haskell}
right us (v:vs)
  =  -- {Por definición de `right` (ec.12.2)}
rep (us ++ [v], vs)
  =  -- {Por (ec.14)}
grep (left (us ++ [v]) vs) (us ++ [v], vs)
\end{minted}

Ahora se busca simplificar \hsCode{left (us ++ [v]) vs}:
\begin{itemize}
\item Caso \texttt{us = []}
\begin{minted}{haskell}
left ([] ++ [v]) vs
  =  -- {Por (++.1)}
left [v] vs
  =  -- {Definición de `left` (ec.11.2)}
rep (split ws []) 
  =  -- {Definición de `split` en (ec.1) porque `e = split ws []`}
rep ([], ws)
  =  -- {Definición de `root` en (ec.13)}
root
\end{minted}

\item Caso inductivo \texttt{(u:us)}
\begin{minted}{haskell}
left (u:us ++ [v]) vs
  =  -- {Definición de `left` (ec.11.2)}
rep (split ws (us ++ [v]))
  =  -- {Definición de `split` (LHS) en (ec.2)}
rep (op (split ws us) v)
  =  -- {Definición de (eq.6) de op' (RHS) y se usa la función `rep`}
op' (rep (split ws us)) v
  =  -- {Definición de `left` (eq.11.2) en (RHS)}
op' (left (u:us) vs) v
\end{minted}

\end{itemize}
\end{itemize}

Logrando así simplificar el argumento de \hsCode{left}. Finalmente quedaría \hsCode{left} como:
\begin{minted}{haskell}
left (us ++ [v]) vs = if null us then root else op' (left us vs) v
\end{minted}

Ergo, \hsCode{grep} puede ser definido como
\begin{minted}{haskell}
grep l (us, [])   = Node(us, []) l Null
grep l (us, v:vs) = Node(us, v:vs) l
                    (grep (op' l v) (us ++ [v], vs))
\end{minted}

\begin{minted}{haskell}
matches :: (Eq a) => [a] -> [a] -> [Int]
matches ws = map fst . filter (ok . snd) . scanl step (0, root)
    where
    ok (Node vs l r) = null vs
    step (n, t) x = (n + 1, op t x)
    op Null x = root
    op (Node [] l r) x = op l x
    op (Node (v:vs) l r) x = if v == x then r else op l x
    root = grep Null ws
    grep l [] = Node [] l Null
    grep l (v:vs) = Node (v : vs) l (grep (op l v) vs))
\end{minted}

% FIXME: poner otras palabras
Ahora pongamos todo juntos.
\begin{minted}{haskell}
matches ws = map fst . filter (ok . snd) . scanl step (0, root)
\end{minted}

donde

\begin{minted}{haskell}
ok (Node (us,vs) l r) = null vs
step (n,t) x = (n+1,op t x)
root = grep Null ([],ws)
\end{minted}

La función \hsCode{op} (que es \hsCode{op'} renombrada) esttá definida como
\begin{minted}{haskell}
op Null x = root
op (Node (us,[]) l r) x = op l x
op (Node (us,v:vs) l r) x = if v == x then r else op l x
\end{minted}

Y la función \hsCode{grep} por
\begin{minted}{haskell}
grep l (us,[]) = Node (us,[]) l Null
grep l (us,v:vs) = Node (us,v :vs) l (grep (op l v) (us ++ [v], vs))
\end{minted}

Juntando todas las definiciones se tiene,

\begin{minted}{haskell}
matches ws = map fst . filter (ok . snd) . scanl step (0, root)
\end{minted}

donde
\begin{minted}{haskell}
ok (Node (us, vs) l r) = null vs
step (n, t) x = (n + 1, op t x)
root = grep Null ([], ws)
\end{minted}

La función \hsCode{op'} se renombró a \hsCode{op}
\begin{minted}{haskell}
op Null x = root
op (Node (us, []) l r) x = op l x
op (Node (us, v:vs) l r) x = if v == x then r else op l x
\end{minted}

y la función \hsCode{grep} por
\begin{minted}{haskell}
grep l (us, []) = Node (us, []) l Null
grep l (us, v:vs) = Node (us, v:vs) l (grep(op l v) (us ++ [v], vs))
\end{minted}

% TODO: explicar mejor
Examinando los lados izquierdos de estas definiciones se ve que la primera entrada \texttt{us} de la tupla \texttt{(us,vs)} no se usa en el algoritmo,
porque su valor nunca es usado. De ahí que, simplemente se desdeña a \texttt{us} y se obtiene el porgrama final como , 

\begin{minted}{haskell}
matches :: (Eq a) => [a] -> [a] -> [Int]
matches ws = map fst . filter (ok . snd) . scanl step (0, root)
    where
    ok (Node vs l r) = null vs
    step (n, t) x = (n + 1, op t x)
    op Null x = root
    op (Node [] l r) x = op l x
    op (Node (v:vs) l r) x = if v == x then r else op l x
    root = grep Null ws
    grep l [] = Node [] l Null
    grep l (v:vs) = Node (v : vs) l (grep (op l v) vs))
\end{minted}

El árbol \texttt{root} es cíclico, % TODO: explicar por que
los subárboles izquieros apuntan de reversa a nodos anteriores in el árbol, o a \hsCode{Null}.
Este árbol encapsula la función de fallo % TODO: cuando explique el algoritmo imperativo, explicar en qué consiste esto,
del algoritmo KMP como una gráfica cíclica. La operación \hsCode{op} toma tiempo constante amortizado, dando por hecho que el costo de la comparación es contante.
El tiempo para calcular \texttt{root} es de $\Theta(m)$ pasos, donde $m =$ \texttt{length ws}. De ahí que \hsCode{matches} toma $\Theta(m)$ pasos para construir \texttt{root}
y a partir de ahí $\Theta(m)$ pasos, donde $n$ es la longitud del texto, para calcular las ocurrencias del patrón en el texto.
\inputminted{haskell}{codigo/haskell/mp-1.hs}
\inputminted{haskell}{codigo/haskell/mp-2.hs}

% TODO: entender en qué constiste eso de poner la función next y si lo puedo hacer. página 134 ver qué onda

%\section{Ejemplos}

ya está la última versión
\inputminted{haskell}{codigo/haskell/kmp.hs}

% Mostrar que mp y kmp son iguales con quickcheck en Putting the ‘K’ into Bird’s derivation of Knuth-Morris-Pratt string matching